{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Dataset/train.txt'\n",
    "val_dir = 'Dataset/val.txt'\n",
    "test_dir = 'Dataset/test.txt'\n",
    "\n",
    "train_dump = 'Dataset/train_clean.txt'\n",
    "val_dump = 'Dataset/val_clean.txt'\n",
    "test_dump = 'Dataset/test_clean.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_dir, 'r', encoding='utf8') as f:\n",
    "    train = f.read()\n",
    "with open(val_dir, 'r', encoding='utf8') as f:\n",
    "    val = f.read()\n",
    "with open(test_dir, 'r', encoding='utf8') as f:\n",
    "    test = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_new = re.sub(r'\\([^ء-ي]*\\)', ' ', train)\n",
    "# arr = re.findall(r'\\([^ء-ي()]*\\)', train)\n",
    "# with open('Dataset/removed.txt', 'w', encoding='utf8') as f:\n",
    "#     for item in arr:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(dataset_str):\n",
    "    # اب (المعجم الوسيط للمعاني) يشسيش\n",
    "    brackets = r'\\([^ء-ي]*\\)'\n",
    "    digits = r'\\d+'\n",
    "    end = r'\\n'\n",
    "    spaces = r'\\s+'\n",
    "    # dash_space_digits = r'-\\s*\\d+\\s*-|-\\s*\\d+\\s*|\\s*\\d+\\s*-'\n",
    "    # slashes = r'(?<!(\\(|\\)|\\.|\\،)) / (?!(\\(|\\)|\\.|\\،))'\n",
    "    stars = r'\\*+'\n",
    "    english_semicolon = r';'\n",
    "    english_comma = r','\n",
    "    long_dash = r'–'\n",
    "    tilde = r'~'\n",
    "    backtick = r'`'\n",
    "    strange_quote = r'“'\n",
    "    strange_quote2 = r'’'\n",
    "    ampersand = r'&'\n",
    "    underscore = r'_'\n",
    "    plus = r'\\+'\n",
    "    equal = r'='\n",
    "    misra_l = r'﴾'\n",
    "    misra_r = r'﴿'\n",
    "    idk = r'\\u200d'\n",
    "    idk2 = r'\\u200f'\n",
    "    dot = r'…'\n",
    "    dot_awi = r'\\.{2,}'\n",
    "    single_quote = r'\\''\n",
    "    shadda_skoon = r'[\\u0651][\\u0652]'\n",
    "\n",
    "    dataset_str = re.sub(brackets, ' ', dataset_str)\n",
    "    # dataset_str = re.sub(dash_space_digits, ' ', dataset_str)\n",
    "    dataset_str = re.sub(digits, ' ', dataset_str)\n",
    "    dataset_str = re.sub(end, ' ', dataset_str)\n",
    "    dataset_str = re.sub(stars, ' ', dataset_str)\n",
    "    dataset_str = re.sub(tilde, ' ', dataset_str)\n",
    "    dataset_str = re.sub(backtick, ' ', dataset_str)\n",
    "    dataset_str = re.sub(ampersand, ' ', dataset_str)\n",
    "    dataset_str = re.sub(underscore, ' ', dataset_str)\n",
    "    dataset_str = re.sub(plus, ' ', dataset_str)\n",
    "    dataset_str = re.sub(equal, ' ', dataset_str)\n",
    "    dataset_str = re.sub(strange_quote, ' ', dataset_str)\n",
    "    dataset_str = re.sub(strange_quote2, ' ', dataset_str)\n",
    "    dataset_str = re.sub(dot, ' ', dataset_str)\n",
    "    dataset_str = re.sub(dot_awi, ' ', dataset_str)\n",
    "    dataset_str = re.sub(single_quote, ' ', dataset_str)\n",
    "\n",
    "    # dataset_str = re.sub(slashes, ' ', dataset_str)\n",
    "\n",
    "    dataset_str = re.sub(english_semicolon, ' ؛ ', dataset_str)\n",
    "    dataset_str = re.sub(long_dash, ' - ', dataset_str)\n",
    "    dataset_str = re.sub(misra_l, ' \" ', dataset_str)\n",
    "    dataset_str = re.sub(misra_r, ' \" ', dataset_str)\n",
    "    dataset_str = re.sub(idk, '', dataset_str)\n",
    "    dataset_str = re.sub(idk2, '', dataset_str)\n",
    "    dataset_str = re.sub(shadda_skoon, r'\\u0651', dataset_str)\n",
    "    dataset_str = re.sub(english_comma, ' ، ', dataset_str)\n",
    "\n",
    "    dataset_str = re.sub(r'\\s+\\.', '.', dataset_str)\n",
    "    dataset_str = re.sub(r'\\.\\s+', '.', dataset_str)\n",
    "\n",
    "    dataset_str = re.sub(spaces, r' ', dataset_str)\n",
    "\n",
    "    return dataset_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = clean(train)\n",
    "val_clean = clean(val)\n",
    "test_clean = clean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"', '-', ':', '/', '(', '[', ']', '.', '،', '؛', '{', '}', '!', '»', '؟', ')', '«'}\n",
      "{'-', ':', '/', '(', '}', '{', '.', '؛', '،', '[', ']', '!', '»', '؟', ')', '«'}\n"
     ]
    }
   ],
   "source": [
    "non_arabic = set(re.findall(r'[^ء-يًٌٍَُِّْ\\s]', train_clean))\n",
    "non_arabic_test = set(re.findall(r'[^ء-يًٌٍَُِّْ\\s]', test_clean))\n",
    "\n",
    "print(non_arabic)\n",
    "print(non_arabic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th = r'.{20} / .{20}'\n",
    "# th = r'.{4}(?<!(\\(|\\)|\\.|\\،)) / (?!(\\(|\\)|\\.|\\،)).{4}'\n",
    "# ans = [m.group() for m in re.finditer(th, train_clean)]\n",
    "# for i in ans:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between_slashes = re.findall(r'/[^/]*/', train_clean)\n",
    "\n",
    "# with open('Dataset/between.txt', 'w', encoding='utf8') as f:\n",
    "#     for item in between_slashes:\n",
    "#         f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_clean = re.sub(r'\\.', '.\\n', train_clean)\n",
    "# val_clean = re.sub(r'\\.', '.\\n', val_clean)\n",
    "\n",
    "with open(train_dump, 'w', encoding='utf8') as f:\n",
    "    f.write(train_clean)\n",
    "with open(val_dump, 'w', encoding='utf8') as f:\n",
    "    f.write(val_clean)\n",
    "with open(test_dump, 'w', encoding='utf8') as f:\n",
    "    f.write(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening = re.findall(r'\\(', train_clean)\n",
    "# closing = re.findall(r'\\)', train_clean)\n",
    "# matched = re.findall(r'\\([^()]*\\)', train_clean)\n",
    "# print(len(matched))\n",
    "# print(len(opening))\n",
    "# print(len(closing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ :\n",
      "Line fady\n",
      "clean  قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ ابْنُ عَرَفَةَ : قَوْلُهُ : \n"
     ]
    }
   ],
   "source": [
    "# train:        dh kda bl 3ak b kol 7aga\n",
    "# train clean:  dh kda mn 8eir el 3ak el kteer dh. bs bl diacritics 3ady\n",
    "print(\"train \", train[:100])\n",
    "print(\"Line fady\")\n",
    "print(\"clean \", train_clean[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ayza tet7at fy trainer copy aw 7aga b3d kda 3lshan teshta8al\n",
    "# w yet7at m3aha function clean brdo xD\n",
    "\n",
    "def Test(test_sentences, model, sentence_encoder, max_len = 500):\n",
    "    # test_sentences: el text el kbeer bta3 el test (kaza gomla 3ady b dots b 3ak kteer kda)\n",
    "    # model: el model el hn-predict byh\n",
    "    # sentence_encoder: el encoder el hn-convert el text le numbers\n",
    "    test_sentences_clean = clean(test_sentences)\n",
    "    test_sentences_clean = test_sentences_clean.split('.')\n",
    "    # split on \\n as well, la mlhash lazma\n",
    "    \n",
    "    for test in test_sentences_clean:\n",
    "        if test == '':      # fy gomal fyha . w \\n, fa hyb2a fady, bnshelha\n",
    "            test_sentences_clean.remove(test)\n",
    "    # split 3la no2ta wa7da msh aktr, w \\n \n",
    "    # kda kda el clean bysheel el kaza no2ta\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sentence1 = ''\n",
    "    sentence2 = ''\n",
    "    label2 = []\n",
    "    # arabic_letters = {'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س','ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي'}\n",
    "    \n",
    "    for sentence in test_sentences_clean:\n",
    "        sentence = SOS + sentence + EOS\n",
    "        for j in range(0, len(sentence), max_len//2):\n",
    "            current_sentence = sentence[j:j+max_len]\n",
    "            \n",
    "            current_sentence, labels = clamp_sentence(current_sentence, np.full((len(current_sentence)), diacritic2id[''], dtype = np.uint8), max_len)\n",
    "            test_sentence_clean = encode_sentence(current_sentence)\n",
    "            test_sentence_clean = sentence_encoder.transform(test_sentence_clean.reshape(-1)).reshape(1, -1)\n",
    "            \n",
    "            test_sentence_clean = torch.tensor(test_sentence_clean, dtype=torch.int32).to(device)\n",
    "            outputs = model(test_sentence_clean)\n",
    "            _, pred = torch.max(outputs.data, 2)\n",
    "            pred = pred.cpu().numpy().reshape(-1)\n",
    "\n",
    "            for i in range(len(current_sentence)):\n",
    "                sentence1 += current_sentence[i]\n",
    "                sentence1 += id2diacritic[pred[i]]\n",
    "            sentence1 += '\\n'\n",
    "\n",
    "            \n",
    "            if j == 0:\n",
    "                flag = np.array([ord('ء') <= ord(c) <= ord('ي') for c in current_sentence])\n",
    "\n",
    "                current_sentence = np.array(list(current_sentence))\n",
    "                pred = pred[flag]\n",
    "                \n",
    "            else:\n",
    "                flag = np.array([ord('ء') <= ord(c) <= ord('ي') for c in current_sentence[max_len//2:]])\n",
    "\n",
    "                current_sentence = np.array(list(current_sentence[max_len//2:]))\n",
    "                pred = pred[max_len//2:][flag]\n",
    "                \n",
    "            \n",
    "            current_sentence = current_sentence[flag]\n",
    "            sentence2 += ''.join(current_sentence)\n",
    "            label2 += list(pred)\n",
    "\n",
    "    id2 = np.arange(len(sentence2))\n",
    "            \n",
    "    return sentence1[:-1], sentence2, label2, id2\n",
    "# model = torch.load('model_conv.ckpt', map_location=torch.device('cpu'))       # 3lshan t-load el model 3l cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'Test/test_no_diacritics.txt'\n",
    "with open(test_path, 'r', encoding='utf8') as f:\n",
    "    test = f.read()\n",
    "    \n",
    "sentence1, sentence2, label2, id2 = Test(test, model, sentence_encoder)\n",
    "print(len(sentence1))\n",
    "print(len(sentence2))\n",
    "print(len(label2))\n",
    "print(len(id2))\n",
    "print(sentence2[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_output = np.array([0, 6, 0, 4, 6, 0, 4, 14, 4, 4, 14, 6, 0, 6, 4, 0, 6, 2, 0, 12, 0, 14, 6, 0, 4, 14, 0, 0, 6, 0, 0, 0, 14, 14, 8, 6, 0, 0, 2, 0, 6, 0, 6, 2, 0, 0, 6, 1, 14, 4, 6, 14, 6, 0, 4, 14, 4, 4, 14, 2, 0, 14, 4, 4, 14, 14, 8, 6, 4, 0, 6, 0, 6, 0, 0, 14, 0, 14, 0, 0, 2, 0, 0, 14, 0, 6, 5, 14, 0, 0, 0, 4, 6, 0, 2, 0, 6, 0, 6, 2, 0, 0, 4, 14, 1, 14, 0, 4, 6, 0, 6, 0, 0, 2, 0, 6, 0, 6, 2, 0, 0, 4, 14, 1, 14, 4, 0, 6, 4, 0, 0, 14, 0, 4, 14, 6, 0, 4, 14, 4, 2, 6, 2, 6, 14, 6, 0, 14, 8, 0, 14, 6, 0, 6, 4, 6, 4, 10, 14, 14, 8, 6, 0, 14, 4, 10, 0, 2, 8, 2, 14, 6, 0, 0, 14, 0, 14, 14, 6, 0, 6, 2])\n",
    "\n",
    "x = np.array(label2[:len(golden_output)])\n",
    "# for i in range(len(x)):\n",
    "#     print(f'{x[i]} {golden_output[i]}')\n",
    "print(f'{sum(x==golden_output)} correct out of {len(x)}')\n",
    "print(sum(x==golden_output)/len(x) * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417359\n",
      "ل\n"
     ]
    }
   ],
   "source": [
    "# bymawet el gehaz\n",
    "# # write output of sentence2 in an excel file\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({'id': id2, 'output': sentence2}) \n",
    "# df.to_csv('Test/output.csv', index=False, encoding='utf-8')\n",
    "with open('Test/eztxt.txt', 'r', encoding='utf8') as f:\n",
    "    ez = f.read()\n",
    "ez = ez.split('\\n')\n",
    "print(len(ez))\n",
    "# join ez to be one string\n",
    "ez = ''.join(ez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2_np = np.array(list(sentence2))\n",
    "ez_np = np.array(list(ez))\n",
    "\n",
    "print(f'{sum(ez_np == sentence2_np)} correct out of {len(ez)}')\n",
    "print(sum(ez_np==sentence2_np)/len(ez_np) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ez_np[:10])\n",
    "print(sentence2_np[:10])\n",
    "print(sum(ez_np[0:] == sentence2_np[0:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
