{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,\\\n",
    "    Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "# Define the corpus\n",
    "# corpus = ['The cat sat on the mat',\n",
    "#           'The dog ran in the park',\n",
    "#           'The bird sang in the tree']\n",
    "train_corpus = open(\"dataset/train_clean.txt\", \"r\",encoding='utf8').read().split(\".\")\n",
    "train_noDiactric, train_labels =extract_data(train_corpus)\n",
    "# Convert the corpus to a sequence of integers\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "tokenizer.fit_on_texts(train_noDiactric)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_noDiactric)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx['UKN']= len(word2idx) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=tokenizer.word_counts\n",
    "least_10_words = sorted(freq.items(), key=lambda x: x[1])[:10]\n",
    "# print(least_10_words)\n",
    "for word, _ in least_10_words:\n",
    "    # print(word, freq)\n",
    "    del word2idx[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_corpus = open(\"dataset/val_clean.txt\", \"r\",encoding='utf8').read().split(\".\")\n",
    "val_noDiactric, val_labels =extract_data(val_corpus)\n",
    "# Convert the corpus to a sequence of integers\n",
    "val_sequences = tokenizer.texts_to_sequences(val_noDiactric)\n",
    "# val_sequences= []\n",
    "# for i,sentence in enumerate(val_noDiactric):\n",
    "#     val_sequences.append(list())\n",
    "#     for word in sentence:\n",
    "#         if word not in word2idx:\n",
    "#             val_sequences[i].append(word2idx['UKN'])\n",
    "#         else: \n",
    "#             val_sequences[i].append(word2idx[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      " قوله : ولا تكره ضيافته\n",
      "قوله : ( أو قطع الأول يده إلخ ) قال الزركشي ابن عرفة : قوله : بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة ( كإلقاء مصحف بقذر وشد زنار ) ابن عرفة : قول ابن شاس : أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك ( وسحر ) محمد : قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك : هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب\n",
      "[9, 23, 4333, 26212]\n",
      "[9, 5, 279, 93, 194, 49, 15, 911, 39, 466, 9, 1194, 3657, 41322, 41, 255, 4169, 483, 13, 168, 1752, 4, 163, 1186, 27897, 6115, 41323, 5844, 41324, 39, 466, 65, 39, 2665, 5, 1373, 24303, 46, 17908, 57698, 41325, 2975, 3, 1444, 2163, 9596, 57699, 1199, 20, 21706, 89, 65, 97, 3374, 7, 16446, 1200, 1195, 113, 15, 97, 46, 27898, 30, 529, 8306, 421, 270, 53, 57700]\n"
     ]
    }
   ],
   "source": [
    "print(word2idx['قوله'])\n",
    "print(val_noDiactric[0])\n",
    "print(train_noDiactric[0])\n",
    "print(val_sequences[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 6, 20)             2152140   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 20)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 107607)            2259747   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4411887 (16.83 MB)\n",
      "Trainable params: 4411887 (16.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "# I think change to +2?\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size = len(tokenizer.word_index) + 2\n",
    "embedding_size = 20 # was originally 10\n",
    "window_size = 3 \n",
    " \n",
    "# Generate the context-target pairs\n",
    "contexts = []\n",
    "targets = []\n",
    "for sequence in train_sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        context = sequence[i - window_size:i] +\\\n",
    "            sequence[i + 1:i + window_size + 1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "val_conexts = []\n",
    "val_targets = []\n",
    "for sequence in val_sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        context = sequence[i - window_size:i] +\\\n",
    "            sequence[i + 1:i + window_size + 1]\n",
    "        target = sequence[i]\n",
    "        val_conexts.append(context)\n",
    "        val_targets.append(target)\n",
    "# Convert the contexts and targets to numpy arrays\n",
    "X_train = np.array(contexts)\n",
    "Y_train = np.array(targets)\n",
    "X_val = np.array(val_conexts)\n",
    "Y_val = np.array(val_targets)\n",
    "\n",
    " \n",
    "# Define the CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=2*window_size))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 39.6 GiB for an array with shape (98862, 107604) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_val, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 39.6 GiB for an array with shape (98862, 107604) and data type float32"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=30, batch_size=20, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cbow_weights.h5')\n",
    " \n",
    " \n",
    "# Load the pre-trained weights\n",
    "model.load_weights('cbow_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Get the word embeddings\n",
    "# embeddings = model.get_weights()[0]\n",
    " \n",
    "# # Perform PCA to reduce the dimensionality\n",
    "# # of the embeddings\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings)\n",
    " \n",
    "# # Visualize the embeddings\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# for i, word in enumerate(tokenizer.word_index.keys()):\n",
    "#     x, y = reduced_embeddings[i]\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(word, xy=(x, y), xytext=(5, 2),\n",
    "#                  textcoords='offset points',\n",
    "#                  ha='right', va='bottom')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
