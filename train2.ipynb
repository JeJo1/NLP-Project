{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:53:46.931799: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 23:53:46.931870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 23:53:46.934632: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 23:53:46.947248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 23:53:47.851682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Dataset/train_clean.txt'\n",
    "val_dir = 'Dataset/val_clean.txt'\n",
    "test_dir = 'Dataset/test_clean.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_dir, 'r', encoding='utf8') as f:\n",
    "    train = f.read()\n",
    "with open(val_dir, 'r', encoding='utf8') as f:\n",
    "    val = f.read()\n",
    "with open(test_dir, 'r', encoding='utf8') as f:\n",
    "    test = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pkl_dir/train_no_diacritics.txt', 'rb') as f:\n",
    "    train_no_diacritics = pickle.load(f)\n",
    "with open('pkl_dir/train_labels.txt', 'rb') as f:\n",
    "    train_labels = pickle.load(f)\n",
    "with open('pkl_dir/val_no_diacritics.txt', 'rb') as f:\n",
    "    val_no_diacritics = pickle.load(f)\n",
    "with open('pkl_dir/val_labels.txt', 'rb') as f:\n",
    "    val_labels = pickle.load(f)\n",
    "with open('pkl_dir/test_no_diacritics.txt', 'rb') as f:\n",
    "    test_no_diacritics = pickle.load(f)\n",
    "with open('pkl_dir/test_labels.txt', 'rb') as f:\n",
    "    test_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 600\n",
    "\n",
    "train_sentences, train_diacritics = extract_sentences(train_no_diacritics, train_labels, max_len)\n",
    "val_sentences, val_diacritics = extract_sentences(val_no_diacritics, val_labels, max_len)\n",
    "test_sentences, test_diacritics = extract_sentences(test_no_diacritics, test_labels, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = 10000\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(test_sentences))\n",
    "np.random.shuffle(indices)\n",
    "test_sentences = test_sentences[indices]\n",
    "test_diacritics = test_diacritics[indices]\n",
    "test_sentences = test_sentences[:test_len]\n",
    "test_diacritics = test_diacritics[:test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = LabelEncoder().fit(train_sentences.flatten())\n",
    "X_train = sentence_encoder.transform(train_sentences.flatten()).reshape(train_sentences.shape)\n",
    "X_val = sentence_encoder.transform(val_sentences.flatten()).reshape(val_sentences.shape)\n",
    "X_test = sentence_encoder.transform(test_sentences.flatten()).reshape(test_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_sentences, val_sentences, test_sentences\n",
    "del train_no_diacritics, val_no_diacritics, test_no_diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder().fit(train_diacritics.flatten())\n",
    "y_train = label_encoder.transform(train_diacritics.flatten()).reshape(train_diacritics.shape)\n",
    "y_val = label_encoder.transform(val_diacritics.flatten()).reshape(val_diacritics.shape)\n",
    "y_test = label_encoder.transform(test_diacritics.flatten()).reshape(test_diacritics.shape)\n",
    "\n",
    "y_train = 9 * y_train[:, :, 0] + y_train[:, :, 1]\n",
    "y_val = 9 * y_val[:, :, 0] + y_val[:, :, 1]\n",
    "y_test = 9 * y_test[:, :, 0] + y_test[:, :, 1]\n",
    "\n",
    "label_encoder2 = LabelEncoder().fit(y_train.flatten())\n",
    "y_train = label_encoder2.transform(y_train.flatten()).reshape(y_train.shape)\n",
    "y_val = label_encoder2.transform(y_val.flatten()).reshape(y_val.shape)\n",
    "y_test = label_encoder2.transform(y_test.flatten()).reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_diacritics, val_diacritics, test_diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inputs, kernel_size, channels, activation, is_training, scope):\n",
    "    with tf.name_scope(scope):\n",
    "        # Use TensorFlow 2.x Conv1D layer\n",
    "        conv1d_output = Conv1D(\n",
    "            filters=channels,\n",
    "            kernel_size=kernel_size,\n",
    "            activation=None,  # Activation will be applied separately\n",
    "            padding='same')(inputs)\n",
    "        \n",
    "        # Batch normalization\n",
    "        conv1d_output = BatchNormalization()(conv1d_output, training=is_training)\n",
    "        \n",
    "        # Activation function\n",
    "        conv1d_output = Activation(activation)(conv1d_output)\n",
    "\n",
    "    return conv1d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highwaynet(inputs, scope, units=128, activation1=tf.nn.relu, activation2=tf.nn.sigmoid):\n",
    "    with tf.name_scope(scope):\n",
    "        H = tf.keras.layers.Dense(\n",
    "            units=units,\n",
    "            activation=tf.nn.relu,\n",
    "            name='H')(inputs)\n",
    "        T = tf.keras.layers.Dense(\n",
    "            units=units,\n",
    "            activation=tf.nn.sigmoid,\n",
    "            name='T',\n",
    "            bias_initializer=tf.constant_initializer(-1.0))(inputs)\n",
    "    return H * T + inputs * (1.0 - T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Conv1D, Dense, MaxPooling1D, Bidirectional, GRU\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "class CbhgLayer(Layer):\n",
    "    def __init__(self, K, projections, gru_size, gates, **kwargs):\n",
    "        super(CbhgLayer, self).__init__(**kwargs)\n",
    "        self.K = K\n",
    "        self.projections = projections\n",
    "        self.gru_size = gru_size\n",
    "        self.conv_layers = [Conv1D(self.gru_size, k, activation='relu', padding='same', name=f'conv1d_{k}') for k in range(1, self.K + 1)]\n",
    "        self.gate_num = gates\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Convolution bank\n",
    "        conv_outputs = tf.concat([layer(inputs) for layer in self.conv_layers], axis=-1)\n",
    "\n",
    "        # Maxpooling\n",
    "        maxpool_output = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv_outputs)\n",
    "\n",
    "        # Two projection layers\n",
    "        proj1_output = Conv1D(self.projections[0], 3, activation='relu', padding='same', name='proj_1')(maxpool_output)\n",
    "        proj2_output = Conv1D(self.projections[1], 3, padding='same', name='proj_2')(proj1_output)\n",
    "\n",
    "        # Residual connection\n",
    "        highway_input = proj2_output + inputs\n",
    "\n",
    "        # Handle dimensionality mismatch\n",
    "        if highway_input.shape[2] != self.gru_size:\n",
    "            highway_input = Dense(self.gru_size)(highway_input)\n",
    "\n",
    "        for i in range(self.gate_num):\n",
    "            highway_input = highwaynet(highway_input, f'highway_{i}', self.gru_size)\n",
    "        \n",
    "        rnn_input = highway_input\n",
    "\n",
    "        # Bidirectional RNN\n",
    "        outputs = Bidirectional(GRU(self.gru_size, return_sequences=True))(rnn_input, mask=mask)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], 2 * self.gru_size)  # Adjust this based on your output shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 600, 512)          28672     \n",
      "                                                                 \n",
      " time_distributed_34 (TimeD  (None, 600, 512)          262656    \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 600, 512)          0         \n",
      "                                                                 \n",
      " time_distributed_35 (TimeD  (None, 600, 128)          65664     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 600, 128)          0         \n",
      "                                                                 \n",
      " cbhg_layer_12 (CbhgLayer)   (None, 600, 256)          2230272   \n",
      "                                                                 \n",
      " time_distributed_36 (TimeD  (None, 600, 256)          65792     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_37 (TimeD  (None, 600, 15)           3855      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2656911 (10.14 MB)\n",
      "Trainable params: 2656911 (10.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(sentence_encoder.classes_), 512, input_length=max_len))\n",
    "model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(CbhgLayer(K=16, projections=[128, 128], gru_size=128, gates=1))\n",
    "\n",
    "model.add(TimeDistributed(Dense(256, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(np.unique(y_train).shape[0], activation='softmax')))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 00:04:01.997075: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1469772000 exceeds 10% of free system memory.\n",
      "2023-12-11 00:04:10.661981: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1469772000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "638/638 [==============================] - 465s 729ms/step - loss: 0.4416 - accuracy: 0.8487 - val_loss: 0.4406 - val_accuracy: 0.8492\n",
      "Epoch 2/5\n",
      "638/638 [==============================] - 409s 641ms/step - loss: 0.4310 - accuracy: 0.8498 - val_loss: 0.4314 - val_accuracy: 0.8493\n",
      "Epoch 3/5\n",
      "  9/638 [..............................] - ETA: 10:24 - loss: 0.4041 - accuracy: 0.8578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 00:18:54.305862: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 314572800 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 0/8589606912\n",
      "2023-12-11 00:18:54.305914: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                      6364856320\n",
      "InUse:                      5568103952\n",
      "MaxInUse:                   6439474764\n",
      "NumAllocs:                     3533186\n",
      "MaxAllocSize:               1469772000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-12-11 00:18:54.305942: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2023-12-11 00:18:54.305948: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 66\n",
      "2023-12-11 00:18:54.305953: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 7\n",
      "2023-12-11 00:18:54.305956: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 60, 5\n",
      "2023-12-11 00:18:54.305960: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 81\n",
      "2023-12-11 00:18:54.305964: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1024, 5\n",
      "2023-12-11 00:18:54.305967: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2023-12-11 00:18:54.305971: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2048, 3\n",
      "2023-12-11 00:18:54.305974: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3072, 126\n",
      "2023-12-11 00:18:54.305978: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 15360, 5\n",
      "2023-12-11 00:18:54.305981: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 30208, 1\n",
      "2023-12-11 00:18:54.305985: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 32768, 1\n",
      "2023-12-11 00:18:54.305988: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 27\n",
      "2023-12-11 00:18:54.305992: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 114688, 3\n",
      "2023-12-11 00:18:54.305995: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 131072, 3\n",
      "2023-12-11 00:18:54.305999: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 153600, 1\n",
      "2023-12-11 00:18:54.306002: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 196608, 258\n",
      "2023-12-11 00:18:54.306006: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 262144, 11\n",
      "2023-12-11 00:18:54.306009: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 307200, 2\n",
      "2023-12-11 00:18:54.306012: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 327680, 3\n",
      "2023-12-11 00:18:54.306016: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 393216, 3\n",
      "2023-12-11 00:18:54.306019: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 458752, 3\n",
      "2023-12-11 00:18:54.306023: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 524288, 3\n",
      "2023-12-11 00:18:54.306026: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 3\n",
      "2023-12-11 00:18:54.306030: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 655360, 3\n",
      "2023-12-11 00:18:54.306033: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 720896, 3\n",
      "2023-12-11 00:18:54.306037: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 786432, 3\n",
      "2023-12-11 00:18:54.306040: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 851968, 3\n",
      "2023-12-11 00:18:54.306044: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 917504, 3\n",
      "2023-12-11 00:18:54.306047: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 983040, 3\n",
      "2023-12-11 00:18:54.306051: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1048576, 6\n",
      "2023-12-11 00:18:54.306055: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 2304000, 4\n",
      "2023-12-11 00:18:54.306058: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 3145728, 3\n",
      "2023-12-11 00:18:54.306062: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4915200, 1\n",
      "2023-12-11 00:18:54.306065: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9758400, 1\n",
      "2023-12-11 00:18:54.306069: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 19660800, 40\n",
      "2023-12-11 00:18:54.306072: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 73188000, 1\n",
      "2023-12-11 00:18:54.306076: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 78643200, 4\n",
      "2023-12-11 00:18:54.306080: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 195969600, 2\n",
      "2023-12-11 00:18:54.306083: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 314572800, 3\n",
      "2023-12-11 00:18:54.306087: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1469772000, 2\n",
      "2023-12-11 00:18:54.306222: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 8992587776\n",
      "2023-12-11 00:18:54.306268: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 5568103952\n",
      "2023-12-11 00:18:54.306281: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 9294577664\n",
      "2023-12-11 00:18:54.306287: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 6439474764\n",
      "2023-12-11 00:18:54.306439: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_grad_input_ops.cc:345 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[64,2048,1,600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Conv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[64,2048,1,600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:Conv2DBackpropInput] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Conv2DBackpropInput_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[64,2048,1,600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:Conv2DBackpropInput] name: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, tf.keras.utils.to_categorical(y_train), validation_data=(X_val, tf.keras.utils.to_categorical(y_val)), batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, tf.keras.utils.to_categorical(y_train), validation_data=(X_val, tf.keras.utils.to_categorical(y_val)), batch_size=64, epochs=10)\n",
    "model.save('modeel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, tf.keras.utils.to_categorical(y_train), validation_data=(X_val, tf.keras.utils.to_categorical(y_val)), batch_size=64, epochs=10)\n",
    "model.save('modeel2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, tf.keras.utils.to_categorical(y_train), validation_data=(X_val, tf.keras.utils.to_categorical(y_val)), batch_size=64, epochs=10)\n",
    "model.save('modeel3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
